{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Testing in notebook using files, stored locally and in S3 bucket in workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType , StructField, StructType, IntegerType, DoubleType, LongType\n",
    "from pyspark.sql.functions import udf, col, lit, when, year, month, upper, to_date, dayofmonth, hour, weekofyear, dayofweek, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# setup logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dwh.cfg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AWS configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder\\\n",
    "#         .config(\"spark.jars.packages\",\\\n",
    "#                 \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "#         .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def SAS_to_date_format(date):\n",
    "    if date is not None:\n",
    "        return pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "SAS_to_date_udf = udf(SAS_to_date_format, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rename_columns(table, new_columns):\n",
    "    for original, new in zip(table.columns, new_columns):\n",
    "        table = table.withColumnRenamed(original, new)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = 's3a://source-bucket-1726/'\n",
    "output_data = 's3a://destination-bucket-1726/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created\n"
     ]
    }
   ],
   "source": [
    "#Create Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(\"spark session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore immigration data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark =spark.read.load('./sas_data')\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing immigration table\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing immigration table\")\n",
    "# extracting columns to create immigration table\n",
    "immigration_data = df_spark.select('cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr',\\\n",
    "                             'arrdate', 'depdate', 'i94mode', 'i94visa').distinct()\\\n",
    "                     .withColumn(\"immigration_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+-------+-------+-------+-------+--------------+\n",
      "| cicid| i94yr|i94mon|i94port|i94addr|arrdate|depdate|i94mode|i94visa|immigration_id|\n",
      "+------+------+------+-------+-------+-------+-------+-------+-------+--------------+\n",
      "|  27.0|2016.0|   4.0|    BOS|     MA|20545.0|20549.0|    1.0|    1.0|             0|\n",
      "| 233.0|2016.0|   4.0|    NYC|     NY|20545.0|20551.0|    1.0|    2.0|             1|\n",
      "|1103.0|2016.0|   4.0|    NEW|     NY|20545.0|20553.0|    1.0|    2.0|             2|\n",
      "|1123.0|2016.0|   4.0|    NEW|     PA|20545.0|20552.0|    1.0|    1.0|             3|\n",
      "|1446.0|2016.0|   4.0|    NYC|     NY|20545.0|20551.0|    1.0|    2.0|             4|\n",
      "+------+------+------+-------+-------+-------+-------+-------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+\n",
      "|cic_id|  year|month|city_code|state_code|arrival_date|departure_date|mode|visa|immigration_id|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+\n",
      "|  27.0|2016.0|  4.0|      BOS|        MA|     20545.0|       20549.0| 1.0| 1.0|             0|\n",
      "| 233.0|2016.0|  4.0|      NYC|        NY|     20545.0|       20551.0| 1.0| 2.0|             1|\n",
      "|1103.0|2016.0|  4.0|      NEW|        NY|     20545.0|       20553.0| 1.0| 2.0|             2|\n",
      "|1123.0|2016.0|  4.0|      NEW|        PA|     20545.0|       20552.0| 1.0| 1.0|             3|\n",
      "|1446.0|2016.0|  4.0|      NYC|        NY|     20545.0|       20551.0| 1.0| 2.0|             4|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming columns of immigration_data table\n",
    "new_columns = ['cic_id', 'year', 'month', 'city_code', 'state_code','arrival_date', \\\n",
    "                   'departure_date', 'mode', 'visa']\n",
    "immigration_data = rename_columns(immigration_data, new_columns)\n",
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:data wrangling completed with country as United States \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "|cic_id|  year|month|city_code|state_code|arrival_date|departure_date|mode|visa|immigration_id|      country|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "|  27.0|2016.0|  4.0|      BOS|        MA|  2016-04-01|    2016-04-05| 1.0| 1.0|             0|United States|\n",
      "| 233.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             1|United States|\n",
      "|1103.0|2016.0|  4.0|      NEW|        NY|  2016-04-01|    2016-04-09| 1.0| 2.0|             2|United States|\n",
      "|1123.0|2016.0|  4.0|      NEW|        PA|  2016-04-01|    2016-04-08| 1.0| 1.0|             3|United States|\n",
      "|1446.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             4|United States|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data = immigration_data.withColumn('country', lit('United States'))\n",
    "immigration_data = immigration_data.withColumn('arrival_date', SAS_to_date_udf(col('arrival_date')))                            \n",
    "immigration_data = immigration_data.withColumn('departure_date', SAS_to_date_udf(col('departure_date')))\n",
    "logging.info(\"data wrangling completed with country as United States \")\n",
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- visa: double (nullable = true)\n",
      " |-- immigration_id: long (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created parquet files from immigration table, partitioned by state\n"
     ]
    }
   ],
   "source": [
    "# write immigration table to parquet files partitioned by state_code\n",
    "immigration_data.write.mode(\"overwrite\").partitionBy('state_code')\\\n",
    "                    .parquet(path= output_data + 'immigration_data')          \n",
    "logging.info(\"Created parquet files from immigration table, partitioned by state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing immigration_personal\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing immigration_personal\")\n",
    "# extract columns to create immigration_personal table\n",
    "immigration_personal = df_spark.select('cicid', 'i94cit', 'i94res',\\\n",
    "                                'biryear', 'gender', 'insnum').distinct()\\\n",
    "                                .withColumn(\"immi_personal_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+----------+------+-------+----------------+\n",
      "|   cic_id|citizen_country|residence_country|birth_year|gender|ins_num|immi_personal_id|\n",
      "+---------+---------------+-----------------+----------+------+-------+----------------+\n",
      "|5748907.0|          249.0|            249.0|    1954.0|     M|   null|               0|\n",
      "|5748930.0|          249.0|            249.0|    1958.0|     F|   null|               1|\n",
      "|5749074.0|          251.0|            135.0|    1977.0|     M|   null|               2|\n",
      "|5749388.0|          251.0|            251.0|    1966.0|     F|   null|               3|\n",
      "|5749881.0|          251.0|            251.0|    1961.0|     F|   null|               4|\n",
      "+---------+---------------+-----------------+----------+------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data wrangling to match data model\n",
    "new_columns = ['cic_id', 'citizen_country', 'residence_country',\\\n",
    "               'birth_year', 'gender', 'ins_num']\n",
    "immigration_personal = rename_columns(immigration_personal, new_columns)\n",
    "immigration_personal.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created parquet files from immigration_personal table.\n"
     ]
    }
   ],
   "source": [
    "# write immigration_personal table to parquet files\n",
    "immigration_personal.write.mode(\"overwrite\")\\\n",
    "                 .parquet(path = output_data + 'immigration_personal')\n",
    "logging.info(\"Created parquet files from immigration_personal table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing immigration_airline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------------+-----+--------+---------------+\n",
      "|    cicid|airline|         admnum|fltno|visatype|immi_airline_id|\n",
      "+---------+-------+---------------+-----+--------+---------------+\n",
      "|5749101.0|     DL| 9.492750593E10|00469|      B2|              0|\n",
      "|5749130.0|     AA| 9.496847443E10|02572|      B2|              1|\n",
      "|5749305.0|     UA| 9.500357363E10|00085|      B1|              2|\n",
      "|5750351.0|     KE|5.9513939033E10|00035|      WB|              3|\n",
      "|5750515.0|     AA|5.9555506133E10|00280|      WT|              4|\n",
      "+---------+-------+---------------+-----+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing immigration_airline\")\n",
    "# extract columns to create immigration_airline table\n",
    "immigration_airline = df_spark.select('cicid', 'airline', 'admnum', 'fltno', 'visatype').distinct()\\\n",
    "                     .withColumn(\"immi_airline_id\", monotonically_increasing_id())\n",
    "immigration_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------------+-------------+---------+---------------+\n",
      "|   cic_id|airline|      admin_num|flight_number|visa_type|immi_airline_id|\n",
      "+---------+-------+---------------+-------------+---------+---------------+\n",
      "|5749101.0|     DL| 9.492750593E10|        00469|       B2|              0|\n",
      "|5749130.0|     AA| 9.496847443E10|        02572|       B2|              1|\n",
      "|5749305.0|     UA| 9.500357363E10|        00085|       B1|              2|\n",
      "|5750351.0|     KE|5.9513939033E10|        00035|       WB|              3|\n",
      "|5750515.0|     AA|5.9555506133E10|        00280|       WT|              4|\n",
      "+---------+-------+---------------+-------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns of immigration_airline table\n",
    "new_columns = ['cic_id', 'airline', 'admin_num', 'flight_number', 'visa_type']\n",
    "immigration_airline = rename_columns(immigration_airline, new_columns)\n",
    "immigration_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admin_num: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- immi_airline_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_airline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created parquet files from immigration_airline table.\n"
     ]
    }
   ],
   "source": [
    "# write immigration_airline table to parquet files\n",
    "immigration_airline.write.mode(\"overwrite\")\\\n",
    "                .parquet(path=output_data + 'immigration_airline')\n",
    "logging.info(\"Created parquet files from immigration_airline table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "|cic_id|  year|month|city_code|state_code|arrival_date|departure_date|mode|visa|immigration_id|      country|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "|  27.0|2016.0|  4.0|      BOS|        MA|  2016-04-01|    2016-04-05| 1.0| 1.0|             0|United States|\n",
      "| 233.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             1|United States|\n",
      "|1103.0|2016.0|  4.0|      NEW|        NY|  2016-04-01|    2016-04-09| 1.0| 2.0|             2|United States|\n",
      "|1123.0|2016.0|  4.0|      NEW|        PA|  2016-04-01|    2016-04-08| 1.0| 1.0|             3|United States|\n",
      "|1446.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             4|United States|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+-------------+------------+-----------+-----------+--------------------+\n",
      "|cic_id|  year|month|city_code|state_code|arrival_date|departure_date|mode|visa|immigration_id|      country|arrival_month|arrival_year|arrival_day|day_of_week|arrival_week_of_year|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+-------------+------------+-----------+-----------+--------------------+\n",
      "|  27.0|2016.0|  4.0|      BOS|        MA|  2016-04-01|    2016-04-05| 1.0| 1.0|             0|United States|            4|        2016|          1|          6|                  13|\n",
      "| 233.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             1|United States|            4|        2016|          1|          6|                  13|\n",
      "|1103.0|2016.0|  4.0|      NEW|        NY|  2016-04-01|    2016-04-09| 1.0| 2.0|             2|United States|            4|        2016|          1|          6|                  13|\n",
      "|1123.0|2016.0|  4.0|      NEW|        PA|  2016-04-01|    2016-04-08| 1.0| 1.0|             3|United States|            4|        2016|          1|          6|                  13|\n",
      "|1446.0|2016.0|  4.0|      NYC|        NY|  2016-04-01|    2016-04-07| 1.0| 2.0|             4|United States|            4|        2016|          1|          6|                  13|\n",
      "+------+------+-----+---------+----------+------------+--------------+----+----+--------------+-------------+-------------+------------+-----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Processing for immigration_date season table\n",
    "immigration_data = immigration_data.withColumn('arrival_month',month(immigration_data.arrival_date))\n",
    "immigration_data = immigration_data.withColumn('arrival_year',year(immigration_data.arrival_date))\n",
    "immigration_data = immigration_data.withColumn('arrival_day',dayofmonth(immigration_data.arrival_date))\n",
    "immigration_data = immigration_data.withColumn('day_of_week',dayofweek(immigration_data.arrival_date))\n",
    "immigration_data = immigration_data.withColumn('arrival_week_of_year',weekofyear(immigration_data.arrival_date))\n",
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+------------+-----------+--------------------+\n",
      "|arrival_date|arrival_month|day_of_week|arrival_year|arrival_day|arrival_week_of_year|\n",
      "+------------+-------------+-----------+------------+-----------+--------------------+\n",
      "|  2016-04-20|            4|          4|        2016|         20|                  16|\n",
      "|  2016-04-04|            4|          2|        2016|          4|                  14|\n",
      "|  2016-04-19|            4|          3|        2016|         19|                  16|\n",
      "|  2016-04-11|            4|          2|        2016|         11|                  15|\n",
      "|  2016-04-22|            4|          6|        2016|         22|                  16|\n",
      "+------------+-------------+-----------+------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_date = immigration_data.select('arrival_date','arrival_month','day_of_week','arrival_year',\\\n",
    "                                           'arrival_day','arrival_week_of_year').dropDuplicates()\n",
    "immigration_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- arrival_week_of_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temporary sql table\n",
    "immigration_date.createOrReplaceTempView(\"immigration_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add seasons to immigration_date dimension table\n",
    "immigration_date_season = spark.sql('''SELECT arrival_date,\n",
    "                         arrival_month,\n",
    "                         day_of_week,\n",
    "                         arrival_year,\n",
    "                         arrival_day,\n",
    "                         arrival_week_of_year,\n",
    "                         CASE WHEN arrival_month IN (12, 1, 2) THEN 'winter' \n",
    "                                WHEN arrival_month IN (3, 4, 5) THEN 'spring' \n",
    "                                WHEN arrival_month IN (6, 7, 8) THEN 'summer' \n",
    "                                ELSE 'autumn' \n",
    "                         END AS date_season from immigration_date''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+------------+-----------+--------------------+-----------+\n",
      "|arrival_date|arrival_month|day_of_week|arrival_year|arrival_day|arrival_week_of_year|date_season|\n",
      "+------------+-------------+-----------+------------+-----------+--------------------+-----------+\n",
      "|  2016-04-20|            4|          4|        2016|         20|                  16|     spring|\n",
      "|  2016-04-04|            4|          2|        2016|          4|                  14|     spring|\n",
      "|  2016-04-19|            4|          3|        2016|         19|                  16|     spring|\n",
      "|  2016-04-11|            4|          2|        2016|         11|                  15|     spring|\n",
      "|  2016-04-22|            4|          6|        2016|         22|                  16|     spring|\n",
      "+------------+-------------+-----------+------------+-----------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_date_season.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- arrival_week_of_year: integer (nullable = true)\n",
      " |-- date_season: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_date_season.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created parquet files from immigration_date_season table.\n"
     ]
    }
   ],
   "source": [
    "# write immigration_date dimension table to parquet file partitioned by year and month\n",
    "immigration_date_season.write.mode(\"overwrite\")\\\n",
    "                        .partitionBy(\"arrival_year\", \"arrival_month\").parquet(path= output_data + 'immigration_date')\n",
    "logging.info(\"Created parquet files from immigration_date_season table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore I94_SAS_Labels_Descriptions data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing label descriptions\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing label descriptions\")\n",
    "label_file = os.path.join(\"./I94_SAS_Labels_Descriptions.SAS\")\n",
    "with open(label_file) as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_code = {}\n",
    "for countries in contents[10:298]:\n",
    "    set = countries.split('=')\n",
    "    code, country = set[0].strip(), set[1].strip().strip(\"'\")\n",
    "    country_code[code] = country\n",
    "country_code = spark.createDataFrame(country_code.items(), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created country_code parquet files\n"
     ]
    }
   ],
   "source": [
    "country_code.write.mode(\"overwrite\")\\\n",
    "     .parquet(path = output_data + 'country_code')\n",
    "logging.info(\"Created country_code parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_code = {}\n",
    "for cities in contents[303:962]:\n",
    "    set = cities.split('=')\n",
    "    code, city = set[0].strip(\"\\t\").strip().strip(\"'\"),\\\n",
    "                 set[1].strip('\\t').strip().strip(\"''\")\n",
    "    city_code[code] = city\n",
    "city_code = spark.createDataFrame(city_code.items(), ['code', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                city|\n",
      "+----+--------------------+\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "| DTH|DUTCH HARBOR, AK ...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created city_code parquet files\n"
     ]
    }
   ],
   "source": [
    "city_code.write.mode(\"overwrite\")\\\n",
    "     .parquet(path = output_data + 'city_code')\n",
    "logging.info(\"Created city_code parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_code = {}\n",
    "for states in contents[982:1036]:\n",
    "    set = states.split('=')\n",
    "    code, state = set[0].strip('\\t').strip(\"'\"), set[1].strip().strip(\"'\")\n",
    "    state_code[code] = state\n",
    "state_code = spark.createDataFrame(state_code.items(), ['code', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|code|     state|\n",
      "+----+----------+\n",
      "|  AK|    ALASKA|\n",
      "|  AZ|   ARIZONA|\n",
      "|  AR|  ARKANSAS|\n",
      "|  CA|CALIFORNIA|\n",
      "|  CO|  COLORADO|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created state_code parquet files\n"
     ]
    }
   ],
   "source": [
    "state_code.write.mode(\"overwrite\")\\\n",
    "     .parquet(path = output_data + 'state_code')\n",
    "logging.info(\"Created state_code parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore Temperature data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing temperature_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing temperature_data\")\n",
    "# read temperature data file\n",
    "temperature_data = os.path.join(input_data + 'GlobalLandTemperaturesByCity.csv')\n",
    "df = spark.read.csv(temperature_data, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|\n",
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "|1821-06-01|            25.768|                        2.653|Abilene|United States|\n",
      "|1830-11-01|            13.302|                        2.715|Abilene|United States|\n",
      "|1836-11-01|             8.827|           2.1719999999999997|Abilene|United States|\n",
      "|1846-07-01|28.258000000000003|           2.0069999999999997|Abilene|United States|\n",
      "|1863-04-01|            18.553|           1.1740000000000002|Abilene|United States|\n",
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.where(df['Country'] == 'United States')\n",
    "temperature_data = df.select(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty',\\\n",
    "                     'City', 'Country']).distinct()\n",
    "temperature_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------------+-------+-------------+\n",
      "|      date|          avg_temp|avg_temp_uncertainty|   city|      country|\n",
      "+----------+------------------+--------------------+-------+-------------+\n",
      "|1821-06-01|            25.768|               2.653|Abilene|United States|\n",
      "|1830-11-01|            13.302|               2.715|Abilene|United States|\n",
      "|1836-11-01|             8.827|  2.1719999999999997|Abilene|United States|\n",
      "|1846-07-01|28.258000000000003|  2.0069999999999997|Abilene|United States|\n",
      "|1863-04-01|            18.553|  1.1740000000000002|Abilene|United States|\n",
      "+----------+------------------+--------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_columns = ['date', 'avg_temp', 'avg_temp_uncertainty', 'city', 'country']\n",
    "temperature_data = rename_columns(temperature_data, new_columns)\n",
    "temperature_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------------+-------+-------------+----+-----+\n",
      "|      date|          avg_temp|avg_temp_uncertainty|   city|      country|year|month|\n",
      "+----------+------------------+--------------------+-------+-------------+----+-----+\n",
      "|1821-06-01|            25.768|               2.653|Abilene|United States|1821|    6|\n",
      "|1830-11-01|            13.302|               2.715|Abilene|United States|1830|   11|\n",
      "|1836-11-01|             8.827|  2.1719999999999997|Abilene|United States|1836|   11|\n",
      "|1846-07-01|28.258000000000003|  2.0069999999999997|Abilene|United States|1846|    7|\n",
      "|1863-04-01|            18.553|  1.1740000000000002|Abilene|United States|1863|    4|\n",
      "+----------+------------------+--------------------+-------+-------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data = temperature_data.withColumn('date', to_date(col('date')))\n",
    "temperature_data = temperature_data.withColumn('year', year(temperature_data['date']))\n",
    "temperature_data = temperature_data.withColumn('month', month(temperature_data['date']))\n",
    "temperature_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_temp: string (nullable = true)\n",
      " |-- avg_temp_uncertainty: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created temperature_data parquet files\n"
     ]
    }
   ],
   "source": [
    "# write temperature_data table to parquet files\n",
    "temperature_data.write.mode(\"overwrite\")\\\n",
    "               .parquet(path=output_data + 'temperature_data')\n",
    "logging.info(\"Created temperature_data parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing demographics_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing demographics_data\")\n",
    "# read demographics_data file\n",
    "demographics_data = os.path.join(input_data + 'us-cities-demographics.csv')\n",
    "df = spark.read.format('csv').options(header=True, delimiter=';').load(demographics_data)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------+-----------------+------------------+------------+------------------+------------+\n",
      "|         City|     State|Male Population|Female Population|Number of Veterans|Foreign-born|              Race|demog_pop_id|\n",
      "+-------------+----------+---------------+-----------------+------------------+------------+------------------+------------+\n",
      "| Saint Joseph|  Missouri|          37688|            38408|              5846|        3755|             Asian|           0|\n",
      "| Fort Collins|  Colorado|          80893|            80288|              8425|        9704|Hispanic or Latino|           1|\n",
      "|        Nampa|     Idaho|          45651|            44199|              4736|        6607|             White|           2|\n",
      "|        Davis|California|          33493|            34163|              2176|       13997|Hispanic or Latino|           3|\n",
      "|Redondo Beach|California|          34855|            33330|              3014|       13536|Hispanic or Latino|           4|\n",
      "+-------------+----------+---------------+-----------------+------------------+------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_population = df.select(['City', 'State', 'Male Population', 'Female Population', \\\n",
    "                          'Number of Veterans', 'Foreign-born', 'Race']).distinct().na.drop() \\\n",
    "                          .withColumn(\"demog_pop_id\", monotonically_increasing_id())\n",
    "demographics_population.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------+-----------------+---------------+------------+------------------+------------+\n",
      "|         city|     state|male_population|female_population|num_of_vetarans|foreign_born|              race|demog_pop_id|\n",
      "+-------------+----------+---------------+-----------------+---------------+------------+------------------+------------+\n",
      "| Saint Joseph|  Missouri|          37688|            38408|           5846|        3755|             Asian|           0|\n",
      "| Fort Collins|  Colorado|          80893|            80288|           8425|        9704|Hispanic or Latino|           1|\n",
      "|        Nampa|     Idaho|          45651|            44199|           4736|        6607|             White|           2|\n",
      "|        Davis|California|          33493|            34163|           2176|       13997|Hispanic or Latino|           3|\n",
      "|Redondo Beach|California|          34855|            33330|           3014|       13536|Hispanic or Latino|           4|\n",
      "+-------------+----------+---------------+-----------------+---------------+------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_columns = ['city', 'state', 'male_population', 'female_population', \\\n",
    "               'num_of_vetarans', 'foreign_born', 'race']\n",
    "demographics_population = rename_columns(demographics_population, new_columns)\n",
    "demographics_population.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: string (nullable = true)\n",
      " |-- female_population: string (nullable = true)\n",
      " |-- num_of_vetarans: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- demog_pop_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_population.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created demographics_population parquet files\n"
     ]
    }
   ],
   "source": [
    "# write demographics_population table to parquet files\n",
    "demographics_population.write.mode(\"overwrite\")\\\n",
    "                    .parquet(path=output_data + 'demographics_population')\n",
    "logging.info(\"Created demographics_population parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start processing demographics_stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+----------------------+-------------+\n",
      "|           City|     State|Median Age|Average Household Size|demog_stat_id|\n",
      "+---------------+----------+----------+----------------------+-------------+\n",
      "|       Milpitas|California|      36.8|                  3.32|            0|\n",
      "|Rochester Hills|  Michigan|      41.2|                  2.66|            1|\n",
      "|     Buena Park|California|      35.7|                  3.55|            2|\n",
      "|      Daly City|California|      39.7|                  3.26|   8589934592|\n",
      "|       Longview|     Texas|      36.8|                  2.55|  17179869184|\n",
      "+---------------+----------+----------+----------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start processing demographics_stats\")\n",
    "demographics_stats = df.select(['City', 'State', 'Median Age', 'Average Household Size'])\\\n",
    "                         .distinct().na.drop()\\\n",
    "                         .withColumn(\"demog_stat_id\", monotonically_increasing_id())\n",
    "demographics_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+------------------+-------------+\n",
      "|           city|     state|median_age|avg_household_size|demog_stat_id|\n",
      "+---------------+----------+----------+------------------+-------------+\n",
      "|       Milpitas|California|      36.8|              3.32|            0|\n",
      "|Rochester Hills|  Michigan|      41.2|              2.66|            1|\n",
      "|     Buena Park|California|      35.7|              3.55|            2|\n",
      "|      Daly City|California|      39.7|              3.26|   8589934592|\n",
      "|       Longview|     Texas|      36.8|              2.55|  17179869184|\n",
      "+---------------+----------+----------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Renaming columns of demographics_stats table\n",
    "new_columns = ['city', 'state', 'median_age', 'avg_household_size']\n",
    "demographics_stats = rename_columns(demographics_stats, new_columns)\n",
    "demographics_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+------------------+-------------+\n",
      "|           city|     state|median_age|avg_household_size|demog_stat_id|\n",
      "+---------------+----------+----------+------------------+-------------+\n",
      "|       MILPITAS|CALIFORNIA|      36.8|              3.32|            0|\n",
      "|ROCHESTER HILLS|  MICHIGAN|      41.2|              2.66|            1|\n",
      "|     BUENA PARK|CALIFORNIA|      35.7|              3.55|            2|\n",
      "|      DALY CITY|CALIFORNIA|      39.7|              3.26|   8589934592|\n",
      "|       LONGVIEW|     TEXAS|      36.8|              2.55|  17179869184|\n",
      "+---------------+----------+----------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting all data in city and state columns to Uppercase.\n",
    "demographics_stats = demographics_stats.withColumn('city', upper(col('city')))\n",
    "demographics_stats = demographics_stats.withColumn('state', upper(col('state')))\n",
    "demographics_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: string (nullable = true)\n",
      " |-- avg_household_size: string (nullable = true)\n",
      " |-- demog_stat_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_stats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created demographics_stats parquet files\n"
     ]
    }
   ],
   "source": [
    "# write demographics_stats table to parquet files\n",
    "demographics_stats.write.mode(\"overwrite\")\\\n",
    "                    .parquet(path = output_data + 'demographics_stats')\n",
    "logging.info(\"Created demographics_stats parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Data schema of every dimensional table matches data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = output_data\n",
    "s3_bucket = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: immigration_airline\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admin_num: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- immi_airline_id: long (nullable = true)\n",
      "\n",
      "Table: city_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Table: temperature_data\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_temp: string (nullable = true)\n",
      " |-- avg_temp_uncertainty: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Table: demographics_population\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: string (nullable = true)\n",
      " |-- female_population: string (nullable = true)\n",
      " |-- num_of_vetarans: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- demog_pop_id: long (nullable = true)\n",
      "\n",
      "Table: immigration_date\n",
      "root\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- arrival_week_of_year: integer (nullable = true)\n",
      " |-- date_season: string (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      "\n",
      "Table: immigration_personal\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- citizen_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ins_num: string (nullable = true)\n",
      " |-- immi_personal_id: long (nullable = true)\n",
      "\n",
      "Table: country_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Table: state_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "Table: demographics_stats\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: string (nullable = true)\n",
      " |-- avg_household_size: string (nullable = true)\n",
      " |-- demog_stat_id: long (nullable = true)\n",
      "\n",
      "Table: immigration\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- visa: double (nullable = true)\n",
      " |-- immigration_id: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_dir in s3_bucket.iterdir():\n",
    "    if file_dir.is_dir():\n",
    "        path = str(file_dir)\n",
    "        df = spark.read.parquet(path)\n",
    "        print(\"Table: \" + path.split('/')[-1])\n",
    "        schema = df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. Tables are not empty after running ETL data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: immigration_airline is not empty. It contains total 3096313 records.\n",
      "Table: city_code is not empty. It contains total 659 records.\n",
      "Table: temperature_data is not empty. It contains total 687004 records.\n",
      "Table: demographics_population is not empty. It contains total 2875 records.\n",
      "Table: immigration_date is not empty. It contains total 30 records.\n",
      "Table: immigration_personal is not empty. It contains total 3096313 records.\n",
      "Table: country_code is not empty. It contains total 288 records.\n",
      "Table: state_code is not empty. It contains total 54 records.\n",
      "Table: demographics_stats is not empty. It contains total 588 records.\n",
      "Table: immigration is not empty. It contains total 3096313 records.\n"
     ]
    }
   ],
   "source": [
    "for file_dir in s3_bucket.iterdir():\n",
    "    if file_dir.is_dir():\n",
    "        path = str(file_dir)\n",
    "        df = spark.read.parquet(path)\n",
    "        record_num = df.count()\n",
    "        if record_num <= 0:\n",
    "            raise ValueError(\"This table is empty!\")\n",
    "        else:\n",
    "            print(\"Table: \" + path.split('/')[-1] + f\" is not empty. It contains total {record_num} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_immigration_data view created\n",
      "dim_immigration_personal view created\n",
      "dim_immigration_airline view created\n",
      "dim_immigration_date_season view created\n",
      "country_code view created\n",
      "city_code view created\n",
      "state_code view created\n",
      "dim_temperature_data view created\n",
      "dim_demographics_population view created\n",
      "dim_demographics_stats view created\n"
     ]
    }
   ],
   "source": [
    "#Let's check some things in our data\n",
    "immigration_data.createOrReplaceTempView(\"fact_immigration_data\")\n",
    "print(\"fact_immigration_data view created\")\n",
    "immigration_personal.createOrReplaceTempView(\"dim_immigration_personal\")\n",
    "print(\"dim_immigration_personal view created\")\n",
    "immigration_airline.createOrReplaceTempView(\"dim_immigration_airline\")\n",
    "print(\"dim_immigration_airline view created\")\n",
    "immigration_date_season.createOrReplaceTempView(\"dim_immigration_date_season\")\n",
    "print(\"dim_immigration_date_season view created\")\n",
    "country_code.createOrReplaceTempView(\"country_code\")\n",
    "print(\"country_code view created\")\n",
    "city_code.createOrReplaceTempView(\"city_code\")\n",
    "print(\"city_code view created\")\n",
    "state_code.createOrReplaceTempView(\"state_code\")\n",
    "print(\"state_code view created\")\n",
    "temperature_data.createOrReplaceTempView(\"dim_temperature_data\")\n",
    "print(\"dim_temperature_data view created\")\n",
    "demographics_population.createOrReplaceTempView(\"dim_demographics_population\")\n",
    "print(\"dim_demographics_population view created\")\n",
    "demographics_stats.createOrReplaceTempView(\"dim_demographics_stats\")\n",
    "print(\"dim_demographics_stats view created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we define the following function to check for null values\n",
    "def nullValueIdentify(spark_context, tables_to_check):\n",
    "    \"\"\"\n",
    "    This function performs null value checks on specific columns of given tables received as parameters and raises a ValueError exception when null values are encountered.\n",
    "    It receives the following parameters:\n",
    "    spark_context : spark context where the data quality check is to be performed\n",
    "    tables_to_check: A dictionary containing (table, columns) pairs specifying for each table, which column is to be checked for null values.   \n",
    "    \"\"\"  \n",
    "    for table in tables_to_check:\n",
    "        print(f\"Performing data quality check on table {table}.\")\n",
    "        for column in tables_to_check[table]:\n",
    "            returnedVal = spark_context.sql(f\"\"\"SELECT COUNT(*) as nbr FROM {table} WHERE {column} IS NULL\"\"\")\n",
    "            if returnedVal.head()[0] > 0:\n",
    "                raise ValueError(f\"Data quality check failed! Found NULL values in {column} column!\")\n",
    "        print(f\"Table {table} passed quality check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing data quality check on table fact_immigration_data...\n",
      "Table fact_immigration_data passed quality check.\n",
      "Performing data quality check on table dim_immigration_personal...\n",
      "Table dim_immigration_personal passed quality check.\n",
      "Performing data quality check on table dim_immigration_airline...\n",
      "Table dim_immigration_airline passed quality check.\n",
      "Performing data quality check on table country_code...\n",
      "Table country_code passed quality check.\n",
      "Performing data quality check on table city_code...\n",
      "Table city_code passed quality check.\n",
      "Performing data quality check on table state_code...\n",
      "Table state_code passed quality check.\n",
      "Performing data quality check on table dim_temperature_data...\n",
      "Table dim_temperature_data passed quality check.\n",
      "Performing data quality check on table dim_demographics_population...\n",
      "Table dim_demographics_population passed quality check.\n",
      "Performing data quality check on table dim_demographics_stats...\n",
      "Table dim_demographics_stats passed quality check.\n"
     ]
    }
   ],
   "source": [
    "#dictionary of tables and columns to be checked\n",
    "tables_to_check = { 'fact_immigration_data' : ['cic_id'], 'dim_immigration_personal':['residence_country'],\n",
    "                   'dim_immigration_airline': ['visa_type'], 'country_code':['code','country'] ,\n",
    "                    'city_code':['code','city'] ,'state_code':['code','state'] ,'dim_temperature_data':['date','city'] ,\n",
    "                   'dim_demographics_population':['city','male_population'] ,'dim_demographics_stats':['city','avg_household_size']\n",
    "                  }\n",
    "\n",
    "#We call our function on the spark context\n",
    "nullValueIdentify(spark, tables_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct_city_state|\n",
      "+-------------------+\n",
      "|               6408|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To identify the distinct combinations of city and state in our 'fact_immigration_data' table\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT city_code, state_code) as Distinct_city_state\n",
    "FROM fact_immigration_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
